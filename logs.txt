* 
* ==> Audit <==
* |--------------|---------------------|----------|----------|---------|---------------------|---------------------|
|   Command    |        Args         | Profile  |   User   | Version |     Start Time      |      End Time       |
|--------------|---------------------|----------|----------|---------|---------------------|---------------------|
| update-check |                     | minikube | sudipdas | v1.32.0 | 18 Apr 24 17:28 IST | 18 Apr 24 17:28 IST |
| update-check |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 13:15 IST | 19 Apr 24 13:15 IST |
| start        |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 17:12 IST |                     |
| start        |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 17:15 IST |                     |
| start        |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 17:26 IST | 19 Apr 24 17:29 IST |
| start        |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 17:37 IST | 19 Apr 24 17:37 IST |
| kubectl      | -- get pods -A      | minikube | sudipdas | v1.32.0 | 19 Apr 24 17:38 IST | 19 Apr 24 17:40 IST |
| start        |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 17:44 IST | 19 Apr 24 17:45 IST |
| update-check |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:11 IST | 19 Apr 24 18:11 IST |
| update-check |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:20 IST | 19 Apr 24 18:20 IST |
| start        |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:21 IST | 19 Apr 24 18:21 IST |
| docker-env   | minikube docker-env | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:22 IST | 19 Apr 24 18:22 IST |
| service      | wisecow             | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:26 IST | 19 Apr 24 18:28 IST |
| service      | d wisecow           | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:28 IST | 19 Apr 24 18:28 IST |
| service      | wisecow --url       | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:28 IST | 19 Apr 24 18:29 IST |
| service      | wisecow             | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:31 IST | 19 Apr 24 18:32 IST |
| service      | wisecow --url       | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:32 IST | 19 Apr 24 18:32 IST |
| start        |                     | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:51 IST | 19 Apr 24 18:51 IST |
| docker-env   | minikube docker-env | minikube | sudipdas | v1.32.0 | 19 Apr 24 18:52 IST | 19 Apr 24 18:52 IST |
| docker-env   | minikube docker-env | minikube | sudipdas | v1.32.0 | 19 Apr 24 19:14 IST | 19 Apr 24 19:14 IST |
|--------------|---------------------|----------|----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/04/19 18:51:14
Running on machine: Sudips-MacBook-Air
Binary: Built with gc go1.21.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0419 18:51:14.285004   24940 out.go:296] Setting OutFile to fd 1 ...
I0419 18:51:14.285563   24940 out.go:348] isatty.IsTerminal(1) = true
I0419 18:51:14.285566   24940 out.go:309] Setting ErrFile to fd 2...
I0419 18:51:14.285571   24940 out.go:348] isatty.IsTerminal(2) = true
I0419 18:51:14.285789   24940 root.go:338] Updating PATH: /Users/sudipdas/.minikube/bin
W0419 18:51:14.286993   24940 root.go:314] Error reading config file at /Users/sudipdas/.minikube/config/config.json: open /Users/sudipdas/.minikube/config/config.json: no such file or directory
I0419 18:51:14.292672   24940 out.go:303] Setting JSON to false
I0419 18:51:14.334650   24940 start.go:128] hostinfo: {"hostname":"Sudips-MacBook-Air.local","uptime":21716,"bootTime":1713511158,"procs":408,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.4.1","kernelVersion":"23.4.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"0e98cb20-7f2c-58a4-99c3-5468e12730b6"}
W0419 18:51:14.334791   24940 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0419 18:51:14.340788   24940 out.go:177] 😄  minikube v1.32.0 on Darwin 14.4.1 (arm64)
I0419 18:51:14.349170   24940 notify.go:220] Checking for updates...
I0419 18:51:14.349380   24940 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0419 18:51:14.350413   24940 driver.go:378] Setting default libvirt URI to qemu:///system
I0419 18:51:14.464041   24940 docker.go:122] docker version: linux-26.0.0:Docker Desktop 4.29.0 (145265)
I0419 18:51:14.464342   24940 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0419 18:51:14.838631   24940 info.go:266] docker info: {ID:2c3fbeb4-0205-4bba-90b2-908c6b9dd026 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:79 OomKillDisable:false NGoroutines:111 SystemTime:2024-04-19 13:21:14.813009088 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:26 KernelVersion:6.6.22-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4113563648 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/sudipdas/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/sudipdas/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:/Users/sudipdas/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:/Users/sudipdas/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:/Users/sudipdas/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/sudipdas/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/sudipdas/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/sudipdas/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/sudipdas/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/sudipdas/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0419 18:51:14.842668   24940 out.go:177] ✨  Using the docker driver based on existing profile
I0419 18:51:14.846541   24940 start.go:298] selected driver: docker
I0419 18:51:14.846560   24940 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0419 18:51:14.846612   24940 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0419 18:51:14.846717   24940 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0419 18:51:14.971438   24940 info.go:266] docker info: {ID:2c3fbeb4-0205-4bba-90b2-908c6b9dd026 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:79 OomKillDisable:false NGoroutines:111 SystemTime:2024-04-19 13:21:14.95020238 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:26 KernelVersion:6.6.22-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4113563648 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/sudipdas/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/sudipdas/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:/Users/sudipdas/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:/Users/sudipdas/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:/Users/sudipdas/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/sudipdas/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/Users/sudipdas/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/Users/sudipdas/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/Users/sudipdas/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/sudipdas/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0419 18:51:14.972400   24940 cni.go:84] Creating CNI manager for ""
I0419 18:51:14.972426   24940 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0419 18:51:14.972433   24940 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0419 18:51:14.980509   24940 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0419 18:51:14.985032   24940 cache.go:121] Beginning downloading kic base image for docker with docker
I0419 18:51:14.988502   24940 out.go:177] 🚜  Pulling base image ...
I0419 18:51:14.996554   24940 image.go:79] Checking for docker.io/kicbase/stable:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0419 18:51:14.996748   24940 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0419 18:51:14.996776   24940 preload.go:148] Found local preload: /Users/sudipdas/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4
I0419 18:51:14.996785   24940 cache.go:56] Caching tarball of preloaded images
I0419 18:51:14.997519   24940 preload.go:174] Found /Users/sudipdas/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0419 18:51:14.997544   24940 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0419 18:51:14.997920   24940 profile.go:148] Saving config to /Users/sudipdas/.minikube/profiles/minikube/config.json ...
I0419 18:51:15.057120   24940 image.go:83] Found docker.io/kicbase/stable:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0419 18:51:15.057152   24940 cache.go:144] docker.io/kicbase/stable:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0419 18:51:15.057382   24940 cache.go:194] Successfully downloaded all kic artifacts
I0419 18:51:15.057814   24940 start.go:365] acquiring machines lock for minikube: {Name:mk71f521f79617944ad01e8f0a6d145a249e2e36 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0419 18:51:15.057917   24940 start.go:369] acquired machines lock for "minikube" in 80.708µs
I0419 18:51:15.057952   24940 start.go:96] Skipping create...Using existing machine configuration
I0419 18:51:15.057970   24940 fix.go:54] fixHost starting: 
I0419 18:51:15.058184   24940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0419 18:51:15.123302   24940 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W0419 18:51:15.123333   24940 fix.go:128] unexpected machine state, will restart: <nil>
I0419 18:51:15.127553   24940 out.go:177] 🏃  Updating the running docker "minikube" container ...
I0419 18:51:15.132546   24940 machine.go:88] provisioning docker machine ...
I0419 18:51:15.132576   24940 ubuntu.go:169] provisioning hostname "minikube"
I0419 18:51:15.132966   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:15.186421   24940 main.go:141] libmachine: Using SSH client type: native
I0419 18:51:15.187396   24940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10050ef80] 0x1005116f0 <nil>  [] 0s} 127.0.0.1 51083 <nil> <nil>}
I0419 18:51:15.187403   24940 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0419 18:51:15.345317   24940 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0419 18:51:15.345420   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:15.401530   24940 main.go:141] libmachine: Using SSH client type: native
I0419 18:51:15.401965   24940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10050ef80] 0x1005116f0 <nil>  [] 0s} 127.0.0.1 51083 <nil> <nil>}
I0419 18:51:15.401975   24940 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0419 18:51:15.516943   24940 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0419 18:51:15.516964   24940 ubuntu.go:175] set auth options {CertDir:/Users/sudipdas/.minikube CaCertPath:/Users/sudipdas/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/sudipdas/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/sudipdas/.minikube/machines/server.pem ServerKeyPath:/Users/sudipdas/.minikube/machines/server-key.pem ClientKeyPath:/Users/sudipdas/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/sudipdas/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/sudipdas/.minikube}
I0419 18:51:15.516982   24940 ubuntu.go:177] setting up certificates
I0419 18:51:15.516999   24940 provision.go:83] configureAuth start
I0419 18:51:15.517077   24940 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0419 18:51:15.572555   24940 provision.go:138] copyHostCerts
I0419 18:51:15.573663   24940 exec_runner.go:144] found /Users/sudipdas/.minikube/cert.pem, removing ...
I0419 18:51:15.573922   24940 exec_runner.go:203] rm: /Users/sudipdas/.minikube/cert.pem
I0419 18:51:15.574298   24940 exec_runner.go:151] cp: /Users/sudipdas/.minikube/certs/cert.pem --> /Users/sudipdas/.minikube/cert.pem (1127 bytes)
I0419 18:51:15.574753   24940 exec_runner.go:144] found /Users/sudipdas/.minikube/key.pem, removing ...
I0419 18:51:15.574756   24940 exec_runner.go:203] rm: /Users/sudipdas/.minikube/key.pem
I0419 18:51:15.575000   24940 exec_runner.go:151] cp: /Users/sudipdas/.minikube/certs/key.pem --> /Users/sudipdas/.minikube/key.pem (1679 bytes)
I0419 18:51:15.575315   24940 exec_runner.go:144] found /Users/sudipdas/.minikube/ca.pem, removing ...
I0419 18:51:15.575317   24940 exec_runner.go:203] rm: /Users/sudipdas/.minikube/ca.pem
I0419 18:51:15.575380   24940 exec_runner.go:151] cp: /Users/sudipdas/.minikube/certs/ca.pem --> /Users/sudipdas/.minikube/ca.pem (1082 bytes)
I0419 18:51:15.575657   24940 provision.go:112] generating server cert: /Users/sudipdas/.minikube/machines/server.pem ca-key=/Users/sudipdas/.minikube/certs/ca.pem private-key=/Users/sudipdas/.minikube/certs/ca-key.pem org=sudipdas.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0419 18:51:15.773264   24940 provision.go:172] copyRemoteCerts
I0419 18:51:15.773702   24940 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0419 18:51:15.773746   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:15.827478   24940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51083 SSHKeyPath:/Users/sudipdas/.minikube/machines/minikube/id_rsa Username:docker}
I0419 18:51:15.933381   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0419 18:51:15.958133   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0419 18:51:15.975478   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0419 18:51:15.993590   24940 provision.go:86] duration metric: configureAuth took 476.594125ms
I0419 18:51:15.993600   24940 ubuntu.go:193] setting minikube options for container-runtime
I0419 18:51:15.993770   24940 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0419 18:51:15.993818   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:16.047092   24940 main.go:141] libmachine: Using SSH client type: native
I0419 18:51:16.047544   24940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10050ef80] 0x1005116f0 <nil>  [] 0s} 127.0.0.1 51083 <nil> <nil>}
I0419 18:51:16.047550   24940 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0419 18:51:16.163646   24940 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0419 18:51:16.163668   24940 ubuntu.go:71] root file system type: overlay
I0419 18:51:16.163770   24940 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0419 18:51:16.163882   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:16.220208   24940 main.go:141] libmachine: Using SSH client type: native
I0419 18:51:16.220624   24940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10050ef80] 0x1005116f0 <nil>  [] 0s} 127.0.0.1 51083 <nil> <nil>}
I0419 18:51:16.220676   24940 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0419 18:51:16.347828   24940 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0419 18:51:16.347978   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:16.409891   24940 main.go:141] libmachine: Using SSH client type: native
I0419 18:51:16.410302   24940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10050ef80] 0x1005116f0 <nil>  [] 0s} 127.0.0.1 51083 <nil> <nil>}
I0419 18:51:16.410312   24940 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0419 18:51:16.544876   24940 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0419 18:51:16.544898   24940 machine.go:91] provisioned docker machine in 1.412376791s
I0419 18:51:16.544906   24940 start.go:300] post-start starting for "minikube" (driver="docker")
I0419 18:51:16.544917   24940 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0419 18:51:16.545050   24940 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0419 18:51:16.545121   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:16.609784   24940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51083 SSHKeyPath:/Users/sudipdas/.minikube/machines/minikube/id_rsa Username:docker}
I0419 18:51:16.707210   24940 ssh_runner.go:195] Run: cat /etc/os-release
I0419 18:51:16.711662   24940 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0419 18:51:16.711696   24940 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0419 18:51:16.711705   24940 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0419 18:51:16.711710   24940 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0419 18:51:16.711717   24940 filesync.go:126] Scanning /Users/sudipdas/.minikube/addons for local assets ...
I0419 18:51:16.712199   24940 filesync.go:126] Scanning /Users/sudipdas/.minikube/files for local assets ...
I0419 18:51:16.712270   24940 start.go:303] post-start completed in 167.361375ms
I0419 18:51:16.712363   24940 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0419 18:51:16.712431   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:16.772984   24940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51083 SSHKeyPath:/Users/sudipdas/.minikube/machines/minikube/id_rsa Username:docker}
I0419 18:51:16.861028   24940 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0419 18:51:16.865312   24940 fix.go:56] fixHost completed within 1.807394875s
I0419 18:51:16.865320   24940 start.go:83] releasing machines lock for "minikube", held for 1.807438041s
I0419 18:51:16.865399   24940 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0419 18:51:16.923564   24940 ssh_runner.go:195] Run: cat /version.json
I0419 18:51:16.923643   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:16.924771   24940 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0419 18:51:16.925272   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:16.984996   24940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51083 SSHKeyPath:/Users/sudipdas/.minikube/machines/minikube/id_rsa Username:docker}
I0419 18:51:16.985105   24940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51083 SSHKeyPath:/Users/sudipdas/.minikube/machines/minikube/id_rsa Username:docker}
I0419 18:51:17.074565   24940 ssh_runner.go:195] Run: systemctl --version
I0419 18:51:17.484083   24940 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0419 18:51:17.489249   24940 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0419 18:51:17.506082   24940 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0419 18:51:17.506181   24940 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0419 18:51:17.512911   24940 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0419 18:51:17.512928   24940 start.go:472] detecting cgroup driver to use...
I0419 18:51:17.512941   24940 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0419 18:51:17.513298   24940 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0419 18:51:17.524942   24940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0419 18:51:17.532201   24940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0419 18:51:17.539698   24940 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0419 18:51:17.539774   24940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0419 18:51:17.546860   24940 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0419 18:51:17.553148   24940 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0419 18:51:17.560663   24940 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0419 18:51:17.566940   24940 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0419 18:51:17.573144   24940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0419 18:51:17.579325   24940 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0419 18:51:17.584605   24940 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0419 18:51:17.589669   24940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0419 18:51:17.675995   24940 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0419 18:51:28.015953   24940 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.340136375s)
I0419 18:51:28.015995   24940 start.go:472] detecting cgroup driver to use...
I0419 18:51:28.016042   24940 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0419 18:51:28.016436   24940 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0419 18:51:28.031818   24940 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0419 18:51:28.031947   24940 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0419 18:51:28.042882   24940 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0419 18:51:28.057461   24940 ssh_runner.go:195] Run: which cri-dockerd
I0419 18:51:28.061000   24940 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0419 18:51:28.067307   24940 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0419 18:51:28.080957   24940 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0419 18:51:28.153273   24940 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0419 18:51:28.246037   24940 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0419 18:51:28.246208   24940 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0419 18:51:28.257696   24940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0419 18:51:28.340173   24940 ssh_runner.go:195] Run: sudo systemctl restart docker
I0419 18:51:28.529874   24940 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0419 18:51:28.585673   24940 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0419 18:51:28.643084   24940 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0419 18:51:28.699662   24940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0419 18:51:28.755428   24940 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0419 18:51:28.776507   24940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0419 18:51:28.834574   24940 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0419 18:51:28.896540   24940 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0419 18:51:28.897598   24940 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0419 18:51:28.902379   24940 start.go:540] Will wait 60s for crictl version
I0419 18:51:28.902487   24940 ssh_runner.go:195] Run: which crictl
I0419 18:51:28.905537   24940 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0419 18:51:28.972447   24940 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0419 18:51:28.972537   24940 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0419 18:51:28.997422   24940 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0419 18:51:29.024206   24940 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0419 18:51:29.024732   24940 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0419 18:51:29.243625   24940 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0419 18:51:29.244067   24940 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0419 18:51:29.248175   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0419 18:51:29.302599   24940 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0419 18:51:29.302676   24940 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0419 18:51:29.321719   24940 docker.go:671] Got preloaded images: -- stdout --
thissudip/wisecow:0.0.1
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0419 18:51:29.322167   24940 docker.go:601] Images already preloaded, skipping extraction
I0419 18:51:29.322417   24940 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0419 18:51:29.340376   24940 docker.go:671] Got preloaded images: -- stdout --
thissudip/wisecow:0.0.1
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0419 18:51:29.340398   24940 cache_images.go:84] Images are preloaded, skipping loading
I0419 18:51:29.340784   24940 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0419 18:51:29.450802   24940 cni.go:84] Creating CNI manager for ""
I0419 18:51:29.450818   24940 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0419 18:51:29.451184   24940 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0419 18:51:29.451199   24940 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0419 18:51:29.451408   24940 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0419 18:51:29.451484   24940 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0419 18:51:29.451574   24940 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0419 18:51:29.459939   24940 binaries.go:44] Found k8s binaries, skipping transfer
I0419 18:51:29.460051   24940 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0419 18:51:29.466051   24940 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0419 18:51:29.478502   24940 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0419 18:51:29.489926   24940 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0419 18:51:29.501272   24940 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0419 18:51:29.504737   24940 certs.go:56] Setting up /Users/sudipdas/.minikube/profiles/minikube for IP: 192.168.49.2
I0419 18:51:29.504933   24940 certs.go:190] acquiring lock for shared ca certs: {Name:mk94a2c1303238fd1b28ba317d42e004a2445aad Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0419 18:51:29.505508   24940 certs.go:199] skipping minikubeCA CA generation: /Users/sudipdas/.minikube/ca.key
I0419 18:51:29.506025   24940 certs.go:199] skipping proxyClientCA CA generation: /Users/sudipdas/.minikube/proxy-client-ca.key
I0419 18:51:29.506321   24940 certs.go:315] skipping minikube-user signed cert generation: /Users/sudipdas/.minikube/profiles/minikube/client.key
I0419 18:51:29.506767   24940 certs.go:315] skipping minikube signed cert generation: /Users/sudipdas/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0419 18:51:29.507012   24940 certs.go:315] skipping aggregator signed cert generation: /Users/sudipdas/.minikube/profiles/minikube/proxy-client.key
I0419 18:51:29.507569   24940 certs.go:437] found cert: /Users/sudipdas/.minikube/certs/Users/sudipdas/.minikube/certs/ca-key.pem (1679 bytes)
I0419 18:51:29.507790   24940 certs.go:437] found cert: /Users/sudipdas/.minikube/certs/Users/sudipdas/.minikube/certs/ca.pem (1082 bytes)
I0419 18:51:29.507970   24940 certs.go:437] found cert: /Users/sudipdas/.minikube/certs/Users/sudipdas/.minikube/certs/cert.pem (1127 bytes)
I0419 18:51:29.508149   24940 certs.go:437] found cert: /Users/sudipdas/.minikube/certs/Users/sudipdas/.minikube/certs/key.pem (1679 bytes)
I0419 18:51:29.509505   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0419 18:51:29.525083   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0419 18:51:29.540424   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0419 18:51:29.555263   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0419 18:51:29.570076   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0419 18:51:29.584930   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0419 18:51:29.599758   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0419 18:51:29.613825   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0419 18:51:29.628036   24940 ssh_runner.go:362] scp /Users/sudipdas/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0419 18:51:29.642370   24940 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0419 18:51:29.653256   24940 ssh_runner.go:195] Run: openssl version
I0419 18:51:29.660116   24940 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0419 18:51:29.667532   24940 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0419 18:51:29.669939   24940 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Apr 19 11:59 /usr/share/ca-certificates/minikubeCA.pem
I0419 18:51:29.669971   24940 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0419 18:51:29.674930   24940 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0419 18:51:29.680691   24940 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0419 18:51:29.683296   24940 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0419 18:51:29.688078   24940 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0419 18:51:29.692846   24940 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0419 18:51:29.697554   24940 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0419 18:51:29.703085   24940 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0419 18:51:29.707610   24940 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0419 18:51:29.713439   24940 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0419 18:51:29.713567   24940 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0419 18:51:29.726461   24940 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0419 18:51:29.732560   24940 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0419 18:51:29.732852   24940 kubeadm.go:636] restartCluster start
I0419 18:51:29.732944   24940 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0419 18:51:29.738470   24940 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0419 18:51:29.738574   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0419 18:51:29.799451   24940 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:51082"
I0419 18:51:29.807810   24940 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0419 18:51:29.815279   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:29.815346   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:29.822379   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:29.822384   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:29.822437   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:29.828604   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:30.329681   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:30.329898   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:30.344917   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:30.829718   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:30.830166   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:30.852683   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:31.329688   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:31.329959   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:31.338280   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:31.829743   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:31.830119   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:31.850564   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:32.329726   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:32.330255   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:32.349036   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:32.829714   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:32.830116   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:32.851080   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:33.329728   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:33.330068   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:33.351888   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:33.829730   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:33.830117   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:33.860909   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:34.329331   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:34.329491   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:34.425636   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:34.829404   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:34.829568   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0419 18:51:34.902965   24940 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0419 18:51:35.328775   24940 api_server.go:166] Checking apiserver status ...
I0419 18:51:35.328943   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0419 18:51:35.411035   24940 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/82964/cgroup
W0419 18:51:35.509532   24940 api_server.go:177] unable to find freezer cgroup: sudo egrep ^[0-9]+:freezer: /proc/82964/cgroup: Process exited with status 1
stdout:

stderr:
I0419 18:51:35.509689   24940 ssh_runner.go:195] Run: ls
I0419 18:51:35.521340   24940 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51082/healthz ...
I0419 18:51:35.531366   24940 api_server.go:269] stopped: https://127.0.0.1:51082/healthz: Get "https://127.0.0.1:51082/healthz": EOF
I0419 18:51:35.531425   24940 retry.go:31] will retry after 272.726632ms: state is "Stopped"
I0419 18:51:35.804642   24940 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51082/healthz ...
I0419 18:51:37.812378   24940 api_server.go:279] https://127.0.0.1:51082/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0419 18:51:37.812406   24940 retry.go:31] will retry after 322.99423ms: https://127.0.0.1:51082/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0419 18:51:38.136505   24940 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51082/healthz ...
I0419 18:51:38.142995   24940 api_server.go:279] https://127.0.0.1:51082/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0419 18:51:38.143056   24940 retry.go:31] will retry after 481.020934ms: https://127.0.0.1:51082/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0419 18:51:38.624781   24940 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51082/healthz ...
I0419 18:51:38.633703   24940 api_server.go:279] https://127.0.0.1:51082/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0419 18:51:38.633726   24940 retry.go:31] will retry after 442.457173ms: https://127.0.0.1:51082/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0419 18:51:39.077317   24940 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51082/healthz ...
I0419 18:51:39.087113   24940 api_server.go:279] https://127.0.0.1:51082/healthz returned 200:
ok
I0419 18:51:39.115226   24940 system_pods.go:86] 7 kube-system pods found
I0419 18:51:39.115246   24940 system_pods.go:89] "coredns-5dd5756b68-2xmp5" [55d63d65-d789-4f93-8cc8-d0b1b8e92842] Running
I0419 18:51:39.115257   24940 system_pods.go:89] "etcd-minikube" [cb25f27e-d8b5-42e6-93eb-dce75df75383] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0419 18:51:39.115265   24940 system_pods.go:89] "kube-apiserver-minikube" [829d0707-eec2-40e4-bc16-f6da285a4927] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0419 18:51:39.115272   24940 system_pods.go:89] "kube-controller-manager-minikube" [07f10151-f069-4f0e-a061-98e2c0f21d4c] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0419 18:51:39.115276   24940 system_pods.go:89] "kube-proxy-zvmnw" [e62dbabf-c2c5-457d-bcb9-9e255d337318] Running
I0419 18:51:39.115281   24940 system_pods.go:89] "kube-scheduler-minikube" [b50ceb8c-ece6-4f8a-a295-7598b9ed2bf4] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0419 18:51:39.115286   24940 system_pods.go:89] "storage-provisioner" [78494389-23d5-4d7b-b506-1b9c5b720900] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0419 18:51:39.117072   24940 api_server.go:141] control plane version: v1.28.3
I0419 18:51:39.117086   24940 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0419 18:51:39.117095   24940 kubeadm.go:684] Taking a shortcut, as the cluster seems to be properly configured
I0419 18:51:39.117102   24940 kubeadm.go:640] restartCluster took 9.384455833s
I0419 18:51:39.117109   24940 kubeadm.go:406] StartCluster complete in 9.403886833s
I0419 18:51:39.117131   24940 settings.go:142] acquiring lock: {Name:mk93972c8ee8057b2d5cbef2abb30344dec5a4aa Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0419 18:51:39.117348   24940 settings.go:150] Updating kubeconfig:  /Users/sudipdas/.kube/config
I0419 18:51:39.118769   24940 lock.go:35] WriteFile acquiring /Users/sudipdas/.kube/config: {Name:mk94b4fc0146691c3e5e66e2300d12a142711919 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0419 18:51:39.119163   24940 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0419 18:51:39.119591   24940 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0419 18:51:39.119493   24940 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0419 18:51:39.119767   24940 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0419 18:51:39.119778   24940 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0419 18:51:39.119778   24940 addons.go:69] Setting default-storageclass=true in profile "minikube"
W0419 18:51:39.119782   24940 addons.go:240] addon storage-provisioner should already be in state true
I0419 18:51:39.119983   24940 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0419 18:51:39.120525   24940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0419 18:51:39.120735   24940 host.go:66] Checking if "minikube" exists ...
I0419 18:51:39.122572   24940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0419 18:51:39.126575   24940 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0419 18:51:39.126625   24940 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0419 18:51:39.135153   24940 out.go:177] 🔎  Verifying Kubernetes components...
I0419 18:51:39.150329   24940 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0419 18:51:39.207184   24940 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0419 18:51:39.206046   24940 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0419 18:51:39.207197   24940 addons.go:240] addon default-storageclass should already be in state true
I0419 18:51:39.211226   24940 host.go:66] Checking if "minikube" exists ...
I0419 18:51:39.211269   24940 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0419 18:51:39.211275   24940 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0419 18:51:39.211328   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:39.212623   24940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0419 18:51:39.255381   24940 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0419 18:51:39.255517   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0419 18:51:39.269350   24940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51083 SSHKeyPath:/Users/sudipdas/.minikube/machines/minikube/id_rsa Username:docker}
I0419 18:51:39.269441   24940 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0419 18:51:39.269447   24940 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0419 18:51:39.269504   24940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0419 18:51:39.314899   24940 api_server.go:52] waiting for apiserver process to appear ...
I0419 18:51:39.314980   24940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0419 18:51:39.325046   24940 api_server.go:72] duration metric: took 198.398375ms to wait for apiserver process to appear ...
I0419 18:51:39.325056   24940 api_server.go:88] waiting for apiserver healthz status ...
I0419 18:51:39.325064   24940 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51082/healthz ...
I0419 18:51:39.328214   24940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51083 SSHKeyPath:/Users/sudipdas/.minikube/machines/minikube/id_rsa Username:docker}
I0419 18:51:39.330435   24940 api_server.go:279] https://127.0.0.1:51082/healthz returned 200:
ok
I0419 18:51:39.331851   24940 api_server.go:141] control plane version: v1.28.3
I0419 18:51:39.331856   24940 api_server.go:131] duration metric: took 6.797708ms to wait for apiserver health ...
I0419 18:51:39.331860   24940 system_pods.go:43] waiting for kube-system pods to appear ...
I0419 18:51:39.337128   24940 system_pods.go:59] 7 kube-system pods found
I0419 18:51:39.337137   24940 system_pods.go:61] "coredns-5dd5756b68-2xmp5" [55d63d65-d789-4f93-8cc8-d0b1b8e92842] Running
I0419 18:51:39.337141   24940 system_pods.go:61] "etcd-minikube" [cb25f27e-d8b5-42e6-93eb-dce75df75383] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0419 18:51:39.337158   24940 system_pods.go:61] "kube-apiserver-minikube" [829d0707-eec2-40e4-bc16-f6da285a4927] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0419 18:51:39.337166   24940 system_pods.go:61] "kube-controller-manager-minikube" [07f10151-f069-4f0e-a061-98e2c0f21d4c] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0419 18:51:39.337169   24940 system_pods.go:61] "kube-proxy-zvmnw" [e62dbabf-c2c5-457d-bcb9-9e255d337318] Running
I0419 18:51:39.337172   24940 system_pods.go:61] "kube-scheduler-minikube" [b50ceb8c-ece6-4f8a-a295-7598b9ed2bf4] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0419 18:51:39.337176   24940 system_pods.go:61] "storage-provisioner" [78494389-23d5-4d7b-b506-1b9c5b720900] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0419 18:51:39.337179   24940 system_pods.go:74] duration metric: took 5.315208ms to wait for pod list to return data ...
I0419 18:51:39.337184   24940 kubeadm.go:581] duration metric: took 210.540459ms to wait for : map[apiserver:true system_pods:true] ...
I0419 18:51:39.337192   24940 node_conditions.go:102] verifying NodePressure condition ...
I0419 18:51:39.340545   24940 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0419 18:51:39.340554   24940 node_conditions.go:123] node cpu capacity is 8
I0419 18:51:39.340561   24940 node_conditions.go:105] duration metric: took 3.366292ms to run NodePressure ...
I0419 18:51:39.340568   24940 start.go:228] waiting for startup goroutines ...
I0419 18:51:39.366640   24940 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0419 18:51:39.414986   24940 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0419 18:51:40.015465   24940 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0419 18:51:40.022448   24940 addons.go:502] enable addons completed in 903.0065ms: enabled=[storage-provisioner default-storageclass]
I0419 18:51:40.022479   24940 start.go:233] waiting for cluster config update ...
I0419 18:51:40.022496   24940 start.go:242] writing updated cluster config ...
I0419 18:51:40.023385   24940 ssh_runner.go:195] Run: rm -f paused
I0419 18:51:40.238711   24940 start.go:600] kubectl: 1.30.0, cluster: 1.28.3 (minor skew: 2)
I0419 18:51:40.244472   24940 out.go:177] 
W0419 18:51:40.250670   24940 out.go:239] ❗  /opt/homebrew/bin/kubectl is version 1.30.0, which may have incompatibilities with Kubernetes 1.28.3.
I0419 18:51:40.259501   24940 out.go:177]     ▪ Want kubectl v1.28.3? Try 'minikube kubectl -- get pods -A'
I0419 18:51:40.263582   24940 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Apr 19 13:21:34 minikube cri-dockerd[81681]: time="2024-04-19T13:21:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7a9947c932fa89ff7a6d3976c2da28533cf0e19341d567e6ba5d4a41a4070793/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 19 13:21:34 minikube cri-dockerd[81681]: time="2024-04-19T13:21:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d1d544dfa019acabf6f715ac7ccdf7147bedcc6a441677fe333dea5cb1628f03/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 19 13:21:34 minikube cri-dockerd[81681]: time="2024-04-19T13:21:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fa6a28ae4d1cbf372ff78f97065243e6cef363755362d9a1b9f903df6a9b944c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 19 13:21:34 minikube cri-dockerd[81681]: time="2024-04-19T13:21:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c731cc0289af0722377ccae4367d969dad60dc415510d94d1c1763e58b44e146/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 19 13:21:34 minikube cri-dockerd[81681]: time="2024-04-19T13:21:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ac419d948bf90855bd285b3683d4253cfc2f64b2ce1253931cc426688eab580f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 19 13:21:35 minikube dockerd[81383]: time="2024-04-19T13:21:35.212580042Z" level=info msg="ignoring event" container=d3efeeef97e2825dbd99ad8f75f0917b39680f1784f096a9c754f67d485db6aa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:22:47 minikube cri-dockerd[81681]: time="2024-04-19T13:22:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7bb68def42a6778e0df2a62e2bcced711b500525e9c1f3a55076f253d5e2fa6e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 19 13:22:52 minikube dockerd[81383]: time="2024-04-19T13:22:52.058594383Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:22:52 minikube dockerd[81383]: time="2024-04-19T13:22:52.058734842Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:23:08 minikube dockerd[81383]: time="2024-04-19T13:23:08.828189044Z" level=info msg="ignoring event" container=feb3555765e952486b1f0a48c9ea0bdcac2f9c799996999ea1b86b803b34f2f8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:23:09 minikube dockerd[81383]: time="2024-04-19T13:23:09.784450503Z" level=info msg="ignoring event" container=e7e2199bd2690bbe5b083e6dcade96b54d4a6efee91c08cc47c5dfa9d4309bbb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:23:11 minikube dockerd[81383]: time="2024-04-19T13:23:11.853774795Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:23:11 minikube dockerd[81383]: time="2024-04-19T13:23:11.853850462Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:23:17 minikube dockerd[81383]: time="2024-04-19T13:23:17.820255548Z" level=info msg="ignoring event" container=46fe80fb4476cfbdbdacaf7e1b7c13c5b2082367b941c2fd8a16b329ddd5c738 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:23:42 minikube dockerd[81383]: time="2024-04-19T13:23:42.949201046Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:23:42 minikube dockerd[81383]: time="2024-04-19T13:23:42.949278671Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:24:28 minikube dockerd[81383]: time="2024-04-19T13:24:28.159131678Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:24:28 minikube dockerd[81383]: time="2024-04-19T13:24:28.159268636Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:25:53 minikube dockerd[81383]: time="2024-04-19T13:25:53.551193717Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:25:53 minikube dockerd[81383]: time="2024-04-19T13:25:53.551328592Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:28:18 minikube dockerd[81383]: time="2024-04-19T13:28:18.837954674Z" level=info msg="ignoring event" container=3eaefa1154cb2d1be9ec0e5ae0667bce2c1d3c8412453032bcd26859223690a4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:28:21 minikube dockerd[81383]: time="2024-04-19T13:28:21.808657300Z" level=info msg="ignoring event" container=31b86ae63c306ea86f23d253af525d0bdaabe77e9ca4bb0348d459253eaa40a2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:28:29 minikube dockerd[81383]: time="2024-04-19T13:28:29.807303220Z" level=info msg="ignoring event" container=2900f69a9ba18b33501d8c0610b2b9f21680b55a09955c6a1d70f98cc0b0ea3f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:28:40 minikube dockerd[81383]: time="2024-04-19T13:28:40.981016795Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:28:40 minikube dockerd[81383]: time="2024-04-19T13:28:40.981152336Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:33:27 minikube dockerd[81383]: time="2024-04-19T13:33:27.869770386Z" level=info msg="ignoring event" container=67df3970cd3355a3fe90f5fb470e5d5d916cb7c09c40f38f197dad006c319601 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:33:30 minikube dockerd[81383]: time="2024-04-19T13:33:30.801676596Z" level=info msg="ignoring event" container=3dec079dd8472179acd7dbd01f7a4951f05234c07361f8d1abde4a9f73e49b57 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:33:40 minikube dockerd[81383]: time="2024-04-19T13:33:40.834105920Z" level=info msg="ignoring event" container=d5ac1ab6ca0fc14f85c58a667143c9668a71291cd907f09ab8020dd704e909f4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:33:51 minikube dockerd[81383]: time="2024-04-19T13:33:51.570308758Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:33:51 minikube dockerd[81383]: time="2024-04-19T13:33:51.570425091Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:38:30 minikube dockerd[81383]: time="2024-04-19T13:38:30.825171304Z" level=info msg="ignoring event" container=aeccb19632fd58d007fc215e0444c9e836f68b3d88771eb61c24ae34667408be module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:38:42 minikube dockerd[81383]: time="2024-04-19T13:38:42.779996046Z" level=info msg="ignoring event" container=f93ac4327e0908b9b8e90aabb6e5349f157f2f0dc22defabadba4d021818322f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:38:49 minikube dockerd[81383]: time="2024-04-19T13:38:49.810716841Z" level=info msg="ignoring event" container=cde36dc1021249bb8f635f2fc81b82f232ef7fb97caa98b2fe748f51a941d9d4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:38:56 minikube dockerd[81383]: time="2024-04-19T13:38:56.015110969Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:38:56 minikube dockerd[81383]: time="2024-04-19T13:38:56.015221552Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:43:42 minikube dockerd[81383]: time="2024-04-19T13:43:42.810943421Z" level=info msg="ignoring event" container=1d36c18ae2fdcec09534819917cd4211456bf1d042b4dcac1ce815b7c946ab75 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:43:44 minikube dockerd[81383]: time="2024-04-19T13:43:44.794776463Z" level=info msg="ignoring event" container=7948e56c90c31de0bf812acfc93ca8a60c9a1e2f7a328681418d2d94eaefe52f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:43:57 minikube dockerd[81383]: time="2024-04-19T13:43:57.790692803Z" level=info msg="ignoring event" container=65571dc89146ea2649c13f1bc8d8fe87804b281efab14de01740caceb0c2ef21 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:44:12 minikube dockerd[81383]: time="2024-04-19T13:44:12.982909671Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:44:12 minikube dockerd[81383]: time="2024-04-19T13:44:12.982961712Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:48:49 minikube dockerd[81383]: time="2024-04-19T13:48:49.755568257Z" level=info msg="ignoring event" container=a2dc5219b43f67544eb8eafdaae045601c7dbcbd26c4d85068acddc62e1d9d88 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:48:49 minikube dockerd[81383]: time="2024-04-19T13:48:49.755625091Z" level=info msg="ignoring event" container=04aeb8ef7579a3c5f8702a3ed9bed4d61cce6d48ca3c9268a410a76665083020 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:49:04 minikube dockerd[81383]: time="2024-04-19T13:49:04.697031875Z" level=info msg="ignoring event" container=da9cd3345f0eb5b14a934ff01c8bddd85698f4dc203855d15a5a81b0e4f95bc4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:49:35 minikube dockerd[81383]: time="2024-04-19T13:49:35.365359250Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:49:35 minikube dockerd[81383]: time="2024-04-19T13:49:35.365527917Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:53:53 minikube dockerd[81383]: time="2024-04-19T13:53:53.725976676Z" level=info msg="ignoring event" container=d4360f42e8661deee6676a6958fa0f3a94b1630086683c920b45c5ddd078af2c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:53:57 minikube dockerd[81383]: time="2024-04-19T13:53:57.712821761Z" level=info msg="ignoring event" container=4f60dd1d39a49b0730e92aca88d28ee74d12ba7445699008e73021b6f621a261 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:54:08 minikube dockerd[81383]: time="2024-04-19T13:54:08.722423835Z" level=info msg="ignoring event" container=08aa654106866dfdb8df7218d24d2cbd0ebceed2f42a77aa8693387d59c5a5ce module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:55:07 minikube dockerd[81383]: time="2024-04-19T13:55:07.870390252Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 13:55:07 minikube dockerd[81383]: time="2024-04-19T13:55:07.870541793Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 13:59:04 minikube dockerd[81383]: time="2024-04-19T13:59:04.733917792Z" level=info msg="ignoring event" container=089429279d102f8e1b24a78640d546c7481b3d210940cd3ee0d74ce3bea693b0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:59:05 minikube dockerd[81383]: time="2024-04-19T13:59:05.711139959Z" level=info msg="ignoring event" container=8aa854a9a8dfd7c39c037ca311cff33459b3703a4894a48c8e29f389371b0adc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 13:59:19 minikube dockerd[81383]: time="2024-04-19T13:59:19.703235549Z" level=info msg="ignoring event" container=6a71dacb1100fbfc4811f0ebf6e0bf0777641023eddf9304e01663ff03e7dc27 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 14:00:18 minikube dockerd[81383]: time="2024-04-19T14:00:18.959659549Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 14:00:18 minikube dockerd[81383]: time="2024-04-19T14:00:18.959762132Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 19 14:04:07 minikube dockerd[81383]: time="2024-04-19T14:04:07.714773043Z" level=info msg="ignoring event" container=dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 14:04:16 minikube dockerd[81383]: time="2024-04-19T14:04:16.689393839Z" level=info msg="ignoring event" container=35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 14:04:28 minikube dockerd[81383]: time="2024-04-19T14:04:28.709926428Z" level=info msg="ignoring event" container=8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 19 14:05:26 minikube dockerd[81383]: time="2024-04-19T14:05:26.537124969Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 19 14:05:26 minikube dockerd[81383]: time="2024-04-19T14:05:26.537215719Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
8cc67d87337e1       8d3497c463cb5       4 minutes ago       Exited              wisecow                   18                  d1d544dfa019a       wisecow-54fd489786-mvw87
35b8b88a72bee       8d3497c463cb5       4 minutes ago       Exited              wisecow                   18                  c731cc0289af0       wisecow-54fd489786-s2p8q
dd1435fd6d8e5       8d3497c463cb5       4 minutes ago       Exited              wisecow                   18                  fa6a28ae4d1cb       wisecow-54fd489786-r6wpc
af8b3fbb81c67       ba04bb24b9575       47 minutes ago      Running             storage-provisioner       8                   194fab1a879e0       storage-provisioner
3c627b4d1f265       97e04611ad434       47 minutes ago      Running             coredns                   4                   ac419d948bf90       coredns-5dd5756b68-2xmp5
1fdc13ca9aae9       a5dd5cdd6d3ef       47 minutes ago      Running             kube-proxy                4                   d04d08c53e19f       kube-proxy-zvmnw
6d4180c9e7dcb       9cdd6470f48c8       47 minutes ago      Running             etcd                      4                   7a9947c932fa8       etcd-minikube
55611427cd7ed       42a4e73724daa       47 minutes ago      Running             kube-scheduler            4                   b61db98456a3c       kube-scheduler-minikube
fa5b7d46e6423       537e9a59ee2fd       47 minutes ago      Running             kube-apiserver            4                   0003fefd2ce43       kube-apiserver-minikube
5946352a462f8       8276439b4f237       47 minutes ago      Running             kube-controller-manager   4                   777b334b9321d       kube-controller-manager-minikube
d3efeeef97e28       ba04bb24b9575       47 minutes ago      Exited              storage-provisioner       7                   194fab1a879e0       storage-provisioner
883d5ff5d7ad1       97e04611ad434       About an hour ago   Exited              coredns                   3                   8b2b007cbe266       coredns-5dd5756b68-2xmp5
125182ac28b64       9cdd6470f48c8       About an hour ago   Exited              etcd                      3                   debe36e32cb45       etcd-minikube
7103578ad6617       8276439b4f237       About an hour ago   Exited              kube-controller-manager   3                   6a300597c2a78       kube-controller-manager-minikube
938d3a69f6fdc       537e9a59ee2fd       About an hour ago   Exited              kube-apiserver            3                   43d2bc8687384       kube-apiserver-minikube
d10b386048cae       42a4e73724daa       About an hour ago   Exited              kube-scheduler            3                   fd9ddbc45af14       kube-scheduler-minikube
eb46bcfceec84       a5dd5cdd6d3ef       About an hour ago   Exited              kube-proxy                3                   4de7359513f20       kube-proxy-zvmnw

* 
* ==> coredns [3c627b4d1f26] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] 127.0.0.1:40336 - 34152 "HINFO IN 3571157122932280534.7870280152697465580. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.114862458s

* 
* ==> coredns [883d5ff5d7ad] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] 127.0.0.1:38285 - 26158 "HINFO IN 2144019618750634294.4045313148437278570. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.119476625s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_04_19T17_29_48_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 19 Apr 2024 11:59:45 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 19 Apr 2024 14:08:56 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 19 Apr 2024 14:04:51 +0000   Fri, 19 Apr 2024 11:59:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 19 Apr 2024 14:04:51 +0000   Fri, 19 Apr 2024 11:59:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 19 Apr 2024 14:04:51 +0000   Fri, 19 Apr 2024 11:59:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 19 Apr 2024 14:04:51 +0000   Fri, 19 Apr 2024 11:59:45 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             3923Mi
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             3923Mi
  pods:               110
System Info:
  Machine ID:                 6498fdf71afb4d7aadf56d773118bf5e
  System UUID:                6498fdf71afb4d7aadf56d773118bf5e
  Boot ID:                    e5941053-d17d-4ef3-a719-e6073e2f1b0a
  Kernel Version:             6.6.22-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     wisecow-54fd489786-mvw87            500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (3%!)(MISSING)       128Mi (3%!)(MISSING)     73m
  default                     wisecow-54fd489786-r6wpc            500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (3%!)(MISSING)       128Mi (3%!)(MISSING)     73m
  default                     wisecow-54fd489786-s2p8q            500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (3%!)(MISSING)       128Mi (3%!)(MISSING)     73m
  default                     wisecow-5f4958d5fd-fws4n            500m (6%!)(MISSING)     500m (6%!)(MISSING)   128Mi (3%!)(MISSING)       128Mi (3%!)(MISSING)     46m
  kube-system                 coredns-5dd5756b68-2xmp5            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     128m
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         129m
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         129m
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         129m
  kube-system                 kube-proxy-zvmnw                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         128m
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         129m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         129m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                2750m (34%!)(MISSING)  2 (25%!)(MISSING)
  memory             682Mi (17%!)(MISSING)  682Mi (17%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type     Reason                   Age                  From             Message
  ----     ------                   ----                 ----             -------
  Normal   Starting                 128m                 kube-proxy       
  Normal   Starting                 47m                  kube-proxy       
  Normal   Starting                 77m                  kube-proxy       
  Normal   Starting                 113m                 kube-proxy       
  Normal   Starting                 121m                 kube-proxy       
  Normal   Starting                 129m                 kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  129m (x8 over 129m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    129m (x8 over 129m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     129m (x7 over 129m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  129m                 kubelet          Updated Node Allocatable limit across pods
  Normal   NodeAllocatableEnforced  129m                 kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  129m                 kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    129m                 kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     129m                 kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   Starting                 129m                 kubelet          Starting kubelet.
  Normal   RegisteredNode           129m                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode           120m                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode           113m                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeNotReady             77m                  kubelet          Node minikube status is now: NodeNotReady
  Normal   RegisteredNode           77m                  node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  ContainerGCFailed        47m                  kubelet          rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
  Normal   RegisteredNode           47m                  node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Apr19 12:01] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.297242] netlink: 'init': attribute type 4 has an invalid length.
[  +0.024641] fakeowner: loading out-of-tree module taints kernel.
[Apr19 12:02] systemd[693]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set

* 
* ==> etcd [125182ac28b6] <==
* {"level":"info","ts":"2024-04-19T12:51:45.863599Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":3581}
{"level":"info","ts":"2024-04-19T12:51:45.867301Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-04-19T12:51:45.867434Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2024-04-19T12:51:45.867455Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 4, commit: 3581, applied: 0, lastindex: 3581, lastterm: 4]"}
{"level":"warn","ts":"2024-04-19T12:51:45.871361Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-04-19T12:51:45.87328Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":2628}
{"level":"info","ts":"2024-04-19T12:51:45.876864Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":2943}
{"level":"info","ts":"2024-04-19T12:51:45.877533Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-04-19T12:51:45.949841Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-04-19T12:51:45.950258Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-19T12:51:45.950312Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-04-19T12:51:45.950592Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-04-19T12:51:45.950705Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-19T12:51:45.950781Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-19T12:51:45.950789Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-19T12:51:45.950892Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-04-19T12:51:45.951501Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-04-19T12:51:45.951604Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-19T12:51:45.951632Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-19T12:51:45.95319Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-04-19T12:51:45.953306Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-19T12:51:45.953344Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-04-19T12:51:45.95336Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-19T12:51:45.953367Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-04-19T12:51:47.769296Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2024-04-19T12:51:47.769478Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2024-04-19T12:51:47.769507Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-04-19T12:51:47.769618Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2024-04-19T12:51:47.769653Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-04-19T12:51:47.769673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2024-04-19T12:51:47.769686Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-04-19T12:51:47.771632Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-04-19T12:51:47.771923Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-19T12:51:47.772199Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-19T12:51:47.772239Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-04-19T12:51:47.772266Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-04-19T12:51:47.773309Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-04-19T12:51:47.773338Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-04-19T13:01:47.777592Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3353}
{"level":"info","ts":"2024-04-19T13:01:47.804405Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3353,"took":"25.768375ms","hash":2704466800}
{"level":"info","ts":"2024-04-19T13:01:47.804581Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2704466800,"revision":3353,"compact-revision":2628}
{"level":"info","ts":"2024-04-19T13:06:47.779863Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3629}
{"level":"info","ts":"2024-04-19T13:06:47.784644Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3629,"took":"3.932459ms","hash":2825203765}
{"level":"info","ts":"2024-04-19T13:06:47.784718Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2825203765,"revision":3629,"compact-revision":3353}
{"level":"info","ts":"2024-04-19T13:11:47.78598Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3888}
{"level":"info","ts":"2024-04-19T13:11:47.791009Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":3888,"took":"3.952041ms","hash":4122043261}
{"level":"info","ts":"2024-04-19T13:11:47.791103Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4122043261,"revision":3888,"compact-revision":3629}
{"level":"info","ts":"2024-04-19T13:19:21.798499Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4133}
{"level":"info","ts":"2024-04-19T13:19:21.802663Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4133,"took":"3.096791ms","hash":1529455999}
{"level":"info","ts":"2024-04-19T13:19:21.802736Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1529455999,"revision":4133,"compact-revision":3888}
{"level":"info","ts":"2024-04-19T13:21:17.711322Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-04-19T13:21:17.711638Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-04-19T13:21:17.712562Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-19T13:21:17.712813Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-19T13:21:17.821436Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-19T13:21:17.821666Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-04-19T13:21:17.821778Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-19T13:21:17.826701Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-19T13:21:17.827458Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-19T13:21:17.827488Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [6d4180c9e7dc] <==
* {"level":"info","ts":"2024-04-19T13:21:35.708701Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 5"}
{"level":"info","ts":"2024-04-19T13:21:35.708722Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 5, commit: 5460, applied: 0, lastindex: 5460, lastterm: 5]"}
{"level":"warn","ts":"2024-04-19T13:21:35.710802Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-04-19T13:21:35.71168Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":4133}
{"level":"info","ts":"2024-04-19T13:21:35.714551Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":4480}
{"level":"info","ts":"2024-04-19T13:21:35.716151Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-04-19T13:21:35.71776Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-04-19T13:21:35.71819Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-19T13:21:35.718254Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-04-19T13:21:35.718742Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-04-19T13:21:35.719364Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-19T13:21:35.71948Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-19T13:21:35.719497Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-19T13:21:35.719692Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-04-19T13:21:35.719767Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-04-19T13:21:35.719991Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-19T13:21:35.720097Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-19T13:21:35.721256Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-04-19T13:21:35.721451Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-04-19T13:21:35.721467Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-04-19T13:21:35.72163Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-19T13:21:35.721643Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-19T13:21:36.80908Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2024-04-19T13:21:36.809161Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2024-04-19T13:21:36.809186Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-04-19T13:21:36.809207Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2024-04-19T13:21:36.80922Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-04-19T13:21:36.809231Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2024-04-19T13:21:36.809237Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-04-19T13:21:36.810158Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-04-19T13:21:36.810348Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-19T13:21:36.811043Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-19T13:21:36.811085Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-04-19T13:21:36.811118Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-04-19T13:21:36.81153Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-04-19T13:21:36.811875Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-04-19T13:31:36.824827Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4838}
{"level":"info","ts":"2024-04-19T13:31:36.853744Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4838,"took":"28.106625ms","hash":1027221387}
{"level":"info","ts":"2024-04-19T13:31:36.853846Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1027221387,"revision":4838,"compact-revision":4133}
{"level":"info","ts":"2024-04-19T13:36:36.830299Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5088}
{"level":"info","ts":"2024-04-19T13:36:36.834657Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5088,"took":"3.215834ms","hash":851835063}
{"level":"info","ts":"2024-04-19T13:36:36.834742Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":851835063,"revision":5088,"compact-revision":4838}
{"level":"info","ts":"2024-04-19T13:41:36.832135Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5340}
{"level":"info","ts":"2024-04-19T13:41:36.836588Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5340,"took":"3.570084ms","hash":434896046}
{"level":"info","ts":"2024-04-19T13:41:36.836669Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":434896046,"revision":5340,"compact-revision":5088}
{"level":"info","ts":"2024-04-19T13:46:36.779108Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5593}
{"level":"info","ts":"2024-04-19T13:46:36.783568Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5593,"took":"3.533083ms","hash":2082118927}
{"level":"info","ts":"2024-04-19T13:46:36.783632Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2082118927,"revision":5593,"compact-revision":5340}
{"level":"info","ts":"2024-04-19T13:51:36.779118Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5843}
{"level":"info","ts":"2024-04-19T13:51:36.783865Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5843,"took":"3.481666ms","hash":660735455}
{"level":"info","ts":"2024-04-19T13:51:36.783947Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":660735455,"revision":5843,"compact-revision":5593}
{"level":"info","ts":"2024-04-19T13:56:36.779036Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6096}
{"level":"info","ts":"2024-04-19T13:56:36.783003Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6096,"took":"3.311292ms","hash":1731261837}
{"level":"info","ts":"2024-04-19T13:56:36.783071Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1731261837,"revision":6096,"compact-revision":5843}
{"level":"info","ts":"2024-04-19T14:01:36.779468Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6347}
{"level":"info","ts":"2024-04-19T14:01:36.782821Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6347,"took":"2.703916ms","hash":1917771836}
{"level":"info","ts":"2024-04-19T14:01:36.78293Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1917771836,"revision":6347,"compact-revision":6096}
{"level":"info","ts":"2024-04-19T14:06:36.771364Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6598}
{"level":"info","ts":"2024-04-19T14:06:36.775007Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":6598,"took":"2.814209ms","hash":2523743791}
{"level":"info","ts":"2024-04-19T14:06:36.775104Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2523743791,"revision":6598,"compact-revision":6347}

* 
* ==> kernel <==
*  14:09:01 up  2:07,  0 users,  load average: 3.14, 3.05, 3.19
Linux minikube 6.6.22-linuxkit #1 SMP Fri Mar 29 12:21:27 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [938d3a69f6fd] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0419 13:21:27.617849       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0419 13:21:27.663651       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0419 13:21:27.669519       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0419 13:21:27.756792       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0419 13:21:27.807015       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0419 13:21:27.817678       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0419 13:21:27.826816       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [fa5b7d46e642] <==
* I0419 13:21:37.327578       1 handler.go:232] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0419 13:21:37.327591       1 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0419 13:21:37.327594       1 genericapiserver.go:744] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0419 13:21:37.328026       1 handler.go:232] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0419 13:21:37.328036       1 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0419 13:21:37.344731       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0419 13:21:37.344750       1 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0419 13:21:37.767231       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0419 13:21:37.767334       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0419 13:21:37.767667       1 secure_serving.go:213] Serving securely on [::]:8443
I0419 13:21:37.767715       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0419 13:21:37.767782       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0419 13:21:37.768484       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0419 13:21:37.768619       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0419 13:21:37.768626       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0419 13:21:37.768714       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0419 13:21:37.768741       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0419 13:21:37.768776       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0419 13:21:37.768792       1 available_controller.go:423] Starting AvailableConditionController
I0419 13:21:37.768797       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0419 13:21:37.768828       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0419 13:21:37.768841       1 controller.go:78] Starting OpenAPI AggregationController
I0419 13:21:37.768866       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0419 13:21:37.768875       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0419 13:21:37.768904       1 aggregator.go:164] waiting for initial CRD sync...
I0419 13:21:37.769107       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0419 13:21:37.769183       1 controller.go:116] Starting legacy_token_tracking_controller
I0419 13:21:37.769188       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0419 13:21:37.769236       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0419 13:21:37.769274       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0419 13:21:37.769300       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0419 13:21:37.769692       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0419 13:21:37.769737       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0419 13:21:37.769740       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0419 13:21:37.769771       1 controller.go:134] Starting OpenAPI controller
I0419 13:21:37.769797       1 controller.go:85] Starting OpenAPI V3 controller
I0419 13:21:37.769820       1 naming_controller.go:291] Starting NamingConditionController
I0419 13:21:37.769840       1 establishing_controller.go:76] Starting EstablishingController
I0419 13:21:37.769861       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0419 13:21:37.769868       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0419 13:21:37.769874       1 crd_finalizer.go:266] Starting CRDFinalizer
I0419 13:21:37.900852       1 shared_informer.go:318] Caches are synced for configmaps
I0419 13:21:37.900962       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0419 13:21:37.900968       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0419 13:21:37.901025       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0419 13:21:37.901082       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0419 13:21:37.902623       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0419 13:21:37.902772       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0419 13:21:37.902812       1 aggregator.go:166] initial CRD sync complete...
I0419 13:21:37.902831       1 autoregister_controller.go:141] Starting autoregister controller
I0419 13:21:37.902836       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0419 13:21:37.902843       1 cache.go:39] Caches are synced for autoregister controller
E0419 13:21:37.906501       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0419 13:21:37.907607       1 shared_informer.go:318] Caches are synced for node_authorizer
I0419 13:21:37.920523       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0419 13:21:38.776740       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0419 13:21:50.169000       1 controller.go:624] quota admission added evaluator for: endpoints
I0419 13:21:50.169733       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0419 13:22:47.319620       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0419 13:22:47.343275       1 controller.go:624] quota admission added evaluator for: replicasets.apps

* 
* ==> kube-controller-manager [5946352a462f] <==
* I0419 13:28:34.732023       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="179.75µs"
I0419 13:28:41.726621       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="189.708µs"
I0419 13:28:54.736567       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="283.375µs"
I0419 13:29:09.734249       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="240.125µs"
I0419 13:33:28.226049       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="460.958µs"
I0419 13:33:31.303739       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="468.833µs"
I0419 13:33:41.519557       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="539.5µs"
I0419 13:33:41.720792       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="136.708µs"
I0419 13:33:44.720507       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="363.292µs"
I0419 13:33:55.721182       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="586.292µs"
I0419 13:34:04.728311       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="174.666µs"
I0419 13:34:18.729461       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="704.459µs"
I0419 13:38:31.427246       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="586.417µs"
I0419 13:38:43.668078       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="518.167µs"
I0419 13:38:45.719936       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="204.875µs"
I0419 13:38:50.809610       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="227.459µs"
I0419 13:38:57.721525       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="349.167µs"
I0419 13:39:03.717016       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="150.083µs"
I0419 13:39:09.709502       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="160.625µs"
I0419 13:39:24.722172       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="546.167µs"
I0419 13:43:43.569571       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="752.042µs"
I0419 13:43:45.632320       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="140.25µs"
I0419 13:43:56.715268       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="1.086ms"
I0419 13:43:58.878841       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="321.041µs"
I0419 13:43:59.705080       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="151.542µs"
I0419 13:43:59.908550       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="54.292µs"
I0419 13:44:24.716581       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="480.208µs"
I0419 13:44:38.713590       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="215.583µs"
I0419 13:48:49.924684       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="514.291µs"
I0419 13:48:49.931348       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="126.542µs"
I0419 13:49:03.643857       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="143.209µs"
I0419 13:49:03.658122       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="228.333µs"
I0419 13:49:05.207250       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="230.875µs"
I0419 13:49:18.649874       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="596.875µs"
I0419 13:49:46.640354       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="775.167µs"
I0419 13:49:57.646754       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="390.417µs"
I0419 13:53:54.069439       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="400.167µs"
I0419 13:53:58.174129       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="169.458µs"
I0419 13:54:05.628311       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="124.291µs"
I0419 13:54:09.374753       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="138.916µs"
I0419 13:54:12.637933       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="250.584µs"
I0419 13:54:21.636125       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="108.375µs"
I0419 13:55:19.636359       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="536.625µs"
I0419 13:55:33.628074       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="308.792µs"
I0419 13:59:05.147112       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="711.25µs"
I0419 13:59:06.169304       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="260.917µs"
I0419 13:59:15.619597       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="325.125µs"
I0419 13:59:20.404658       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="185.959µs"
I0419 13:59:21.625410       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="151.625µs"
I0419 13:59:35.626530       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="1.909042ms"
I0419 14:00:30.621038       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="1.37ms"
I0419 14:00:45.617635       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="136.208µs"
I0419 14:04:07.961432       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="494.625µs"
I0419 14:04:17.151131       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="195.75µs"
I0419 14:04:23.611378       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="620.542µs"
I0419 14:04:29.391524       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="525.542µs"
I0419 14:04:30.601983       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="109.875µs"
I0419 14:04:40.604759       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="582.333µs"
I0419 14:05:38.614043       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="1.119166ms"
I0419 14:05:53.599255       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-5f4958d5fd" duration="220.209µs"

* 
* ==> kube-controller-manager [7103578ad661] <==
* I0419 12:55:15.681722       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="97.75µs"
I0419 12:55:15.687846       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="41.5µs"
I0419 12:55:15.693981       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="56.75µs"
I0419 12:55:42.782556       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="934.167µs"
I0419 12:55:43.819844       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="222.833µs"
I0419 12:55:44.844124       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="92.583µs"
I0419 12:55:45.873284       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="205.042µs"
I0419 12:55:46.906006       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="89.334µs"
I0419 12:55:47.935691       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="68.959µs"
I0419 12:55:48.944616       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="69.291µs"
I0419 12:55:49.967160       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="47.625µs"
I0419 12:55:50.987411       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="95.625µs"
I0419 12:55:59.122400       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="81.042µs"
I0419 12:56:01.188363       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="347.125µs"
I0419 12:56:03.259120       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="107.458µs"
I0419 12:56:11.773352       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="216.209µs"
I0419 12:56:13.776417       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="244.042µs"
I0419 12:56:18.777610       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="214.917µs"
I0419 12:56:26.673743       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="107.583µs"
I0419 12:56:29.743542       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="118.459µs"
I0419 12:56:30.776572       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="95.209µs"
I0419 12:56:41.769209       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="662.292µs"
I0419 12:56:43.760929       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="186.958µs"
I0419 12:56:45.761023       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="246.583µs"
I0419 12:57:09.457591       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="240.458µs"
I0419 12:57:13.555359       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="60.5µs"
I0419 12:57:13.564841       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="48.958µs"
I0419 12:57:21.752645       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="213.75µs"
I0419 12:57:27.759175       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="110.208µs"
I0419 12:57:27.774015       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="65.75µs"
I0419 12:58:43.020266       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="731.875µs"
I0419 12:58:44.046207       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="108.042µs"
I0419 12:58:48.155193       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="166.375µs"
I0419 12:58:55.761960       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="835.083µs"
I0419 12:58:56.755959       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="387.083µs"
I0419 12:58:59.753350       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="125.167µs"
I0419 13:01:33.818977       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="692.75µs"
I0419 13:01:35.887629       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="240.667µs"
I0419 13:01:38.940140       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="99.375µs"
I0419 13:01:48.755692       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="213.542µs"
I0419 13:01:49.758609       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="230.5µs"
I0419 13:01:50.755196       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="103.834µs"
I0419 13:06:41.857376       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="422.083µs"
I0419 13:06:41.864268       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="83.708µs"
I0419 13:06:42.905300       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="132.541µs"
I0419 13:06:44.950916       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="132.25µs"
I0419 13:06:53.746564       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="259.042µs"
I0419 13:06:57.744999       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="179.208µs"
I0419 13:11:49.970604       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="1.002417ms"
I0419 13:11:54.051454       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="242.375µs"
I0419 13:11:58.136501       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="367.875µs"
I0419 13:12:05.738552       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="289.083µs"
I0419 13:12:08.735004       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="276.709µs"
I0419 13:12:10.746930       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="734.75µs"
I0419 13:18:05.904665       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="369.458µs"
I0419 13:18:05.911058       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="67.5µs"
I0419 13:18:17.110720       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="214µs"
I0419 13:18:18.746323       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="365.875µs"
I0419 13:18:20.749143       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="162µs"
I0419 13:18:31.750653       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/wisecow-54fd489786" duration="402.208µs"

* 
* ==> kube-proxy [1fdc13ca9aae] <==
* I0419 13:21:35.423632       1 server_others.go:69] "Using iptables proxy"
E0419 13:21:35.510485       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I0419 13:21:37.905709       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0419 13:21:37.934933       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0419 13:21:37.936110       1 server_others.go:152] "Using iptables Proxier"
I0419 13:21:37.936133       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0419 13:21:37.936137       1 server_others.go:438] "Defaulting to no-op detect-local"
I0419 13:21:37.936194       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0419 13:21:37.936493       1 server.go:846] "Version info" version="v1.28.3"
I0419 13:21:37.936514       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0419 13:21:38.001390       1 config.go:188] "Starting service config controller"
I0419 13:21:38.001451       1 shared_informer.go:311] Waiting for caches to sync for service config
I0419 13:21:38.001482       1 config.go:97] "Starting endpoint slice config controller"
I0419 13:21:38.001494       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0419 13:21:38.002014       1 config.go:315] "Starting node config controller"
I0419 13:21:38.002040       1 shared_informer.go:311] Waiting for caches to sync for node config
I0419 13:21:38.103241       1 shared_informer.go:318] Caches are synced for node config
I0419 13:21:38.104106       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0419 13:21:38.104138       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [eb46bcfceec8] <==
* I0419 12:51:45.864385       1 server_others.go:69] "Using iptables proxy"
E0419 12:51:48.760673       1 node.go:130] Failed to retrieve node info: nodes "minikube" is forbidden: User "system:serviceaccount:kube-system:kube-proxy" cannot get resource "nodes" in API group "" at the cluster scope
I0419 12:51:49.908080       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0419 12:51:49.923415       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0419 12:51:49.924496       1 server_others.go:152] "Using iptables Proxier"
I0419 12:51:49.924521       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0419 12:51:49.924525       1 server_others.go:438] "Defaulting to no-op detect-local"
I0419 12:51:49.924601       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0419 12:51:49.924904       1 server.go:846] "Version info" version="v1.28.3"
I0419 12:51:49.924918       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0419 12:51:49.925519       1 config.go:315] "Starting node config controller"
I0419 12:51:49.925535       1 config.go:97] "Starting endpoint slice config controller"
I0419 12:51:49.925581       1 shared_informer.go:311] Waiting for caches to sync for node config
I0419 12:51:49.925582       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0419 12:51:49.925613       1 config.go:188] "Starting service config controller"
I0419 12:51:49.925619       1 shared_informer.go:311] Waiting for caches to sync for service config
I0419 12:51:50.026511       1 shared_informer.go:318] Caches are synced for service config
I0419 12:51:50.026552       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0419 12:51:50.027241       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [55611427cd7e] <==
* I0419 13:21:36.355040       1 serving.go:348] Generated self-signed cert in-memory
W0419 13:21:37.823607       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0419 13:21:37.823630       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0419 13:21:37.823638       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0419 13:21:37.823642       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0419 13:21:37.911929       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0419 13:21:37.911949       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0419 13:21:37.914250       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0419 13:21:37.915067       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0419 13:21:37.915098       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0419 13:21:37.915134       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0419 13:21:38.018138       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [d10b386048ca] <==
* I0419 12:51:46.894302       1 serving.go:348] Generated self-signed cert in-memory
W0419 12:51:48.673268       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0419 12:51:48.673306       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0419 12:51:48.673319       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0419 12:51:48.673330       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0419 12:51:48.763069       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0419 12:51:48.763115       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0419 12:51:48.764978       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0419 12:51:48.765102       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0419 12:51:48.765144       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0419 12:51:48.765468       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0419 12:51:48.865270       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0419 13:21:17.707376       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Apr 19 14:07:13 minikube kubelet[2314]: E0419 14:07:13.578599    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"
Apr 19 14:07:13 minikube kubelet[2314]: E0419 14:07:13.580342    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:07:17 minikube kubelet[2314]: I0419 14:07:17.578455    2314 scope.go:117] "RemoveContainer" containerID="35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167"
Apr 19 14:07:17 minikube kubelet[2314]: E0419 14:07:17.578731    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-s2p8q_default(187e51fc-1b75-4598-b2c5-9ebfeaa9ee62)\"" pod="default/wisecow-54fd489786-s2p8q" podUID="187e51fc-1b75-4598-b2c5-9ebfeaa9ee62"
Apr 19 14:07:21 minikube kubelet[2314]: I0419 14:07:21.577667    2314 scope.go:117] "RemoveContainer" containerID="dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7"
Apr 19 14:07:21 minikube kubelet[2314]: E0419 14:07:21.578181    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-r6wpc_default(65ce0807-36ef-4c61-9fb0-2234ea85357a)\"" pod="default/wisecow-54fd489786-r6wpc" podUID="65ce0807-36ef-4c61-9fb0-2234ea85357a"
Apr 19 14:07:22 minikube kubelet[2314]: W0419 14:07:22.700460    2314 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 19 14:07:22 minikube kubelet[2314]: W0419 14:07:22.703555    2314 machine.go:65] Cannot read vendor id correctly, set empty.
Apr 19 14:07:24 minikube kubelet[2314]: I0419 14:07:24.578345    2314 scope.go:117] "RemoveContainer" containerID="8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2"
Apr 19 14:07:24 minikube kubelet[2314]: E0419 14:07:24.579116    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"
Apr 19 14:07:27 minikube kubelet[2314]: E0419 14:07:27.582154    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:07:30 minikube kubelet[2314]: I0419 14:07:30.580811    2314 scope.go:117] "RemoveContainer" containerID="35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167"
Apr 19 14:07:30 minikube kubelet[2314]: E0419 14:07:30.581163    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-s2p8q_default(187e51fc-1b75-4598-b2c5-9ebfeaa9ee62)\"" pod="default/wisecow-54fd489786-s2p8q" podUID="187e51fc-1b75-4598-b2c5-9ebfeaa9ee62"
Apr 19 14:07:36 minikube kubelet[2314]: I0419 14:07:36.577787    2314 scope.go:117] "RemoveContainer" containerID="dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7"
Apr 19 14:07:36 minikube kubelet[2314]: E0419 14:07:36.578460    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-r6wpc_default(65ce0807-36ef-4c61-9fb0-2234ea85357a)\"" pod="default/wisecow-54fd489786-r6wpc" podUID="65ce0807-36ef-4c61-9fb0-2234ea85357a"
Apr 19 14:07:39 minikube kubelet[2314]: I0419 14:07:39.576493    2314 scope.go:117] "RemoveContainer" containerID="8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2"
Apr 19 14:07:39 minikube kubelet[2314]: E0419 14:07:39.576804    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"
Apr 19 14:07:39 minikube kubelet[2314]: E0419 14:07:39.578708    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:07:41 minikube kubelet[2314]: I0419 14:07:41.578398    2314 scope.go:117] "RemoveContainer" containerID="35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167"
Apr 19 14:07:41 minikube kubelet[2314]: E0419 14:07:41.578717    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-s2p8q_default(187e51fc-1b75-4598-b2c5-9ebfeaa9ee62)\"" pod="default/wisecow-54fd489786-s2p8q" podUID="187e51fc-1b75-4598-b2c5-9ebfeaa9ee62"
Apr 19 14:07:50 minikube kubelet[2314]: I0419 14:07:50.577000    2314 scope.go:117] "RemoveContainer" containerID="dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7"
Apr 19 14:07:50 minikube kubelet[2314]: E0419 14:07:50.577336    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-r6wpc_default(65ce0807-36ef-4c61-9fb0-2234ea85357a)\"" pod="default/wisecow-54fd489786-r6wpc" podUID="65ce0807-36ef-4c61-9fb0-2234ea85357a"
Apr 19 14:07:53 minikube kubelet[2314]: I0419 14:07:53.577634    2314 scope.go:117] "RemoveContainer" containerID="8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2"
Apr 19 14:07:53 minikube kubelet[2314]: E0419 14:07:53.579192    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"
Apr 19 14:07:54 minikube kubelet[2314]: E0419 14:07:54.594744    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:07:55 minikube kubelet[2314]: I0419 14:07:55.577640    2314 scope.go:117] "RemoveContainer" containerID="35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167"
Apr 19 14:07:55 minikube kubelet[2314]: E0419 14:07:55.578239    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-s2p8q_default(187e51fc-1b75-4598-b2c5-9ebfeaa9ee62)\"" pod="default/wisecow-54fd489786-s2p8q" podUID="187e51fc-1b75-4598-b2c5-9ebfeaa9ee62"
Apr 19 14:08:03 minikube kubelet[2314]: I0419 14:08:03.577388    2314 scope.go:117] "RemoveContainer" containerID="dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7"
Apr 19 14:08:03 minikube kubelet[2314]: E0419 14:08:03.577891    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-r6wpc_default(65ce0807-36ef-4c61-9fb0-2234ea85357a)\"" pod="default/wisecow-54fd489786-r6wpc" podUID="65ce0807-36ef-4c61-9fb0-2234ea85357a"
Apr 19 14:08:05 minikube kubelet[2314]: E0419 14:08:05.583729    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:08:07 minikube kubelet[2314]: I0419 14:08:07.576573    2314 scope.go:117] "RemoveContainer" containerID="8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2"
Apr 19 14:08:07 minikube kubelet[2314]: E0419 14:08:07.576906    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"
Apr 19 14:08:10 minikube kubelet[2314]: I0419 14:08:10.576599    2314 scope.go:117] "RemoveContainer" containerID="35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167"
Apr 19 14:08:10 minikube kubelet[2314]: E0419 14:08:10.576970    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-s2p8q_default(187e51fc-1b75-4598-b2c5-9ebfeaa9ee62)\"" pod="default/wisecow-54fd489786-s2p8q" podUID="187e51fc-1b75-4598-b2c5-9ebfeaa9ee62"
Apr 19 14:08:15 minikube kubelet[2314]: I0419 14:08:15.576323    2314 scope.go:117] "RemoveContainer" containerID="dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7"
Apr 19 14:08:15 minikube kubelet[2314]: E0419 14:08:15.576768    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-r6wpc_default(65ce0807-36ef-4c61-9fb0-2234ea85357a)\"" pod="default/wisecow-54fd489786-r6wpc" podUID="65ce0807-36ef-4c61-9fb0-2234ea85357a"
Apr 19 14:08:17 minikube kubelet[2314]: E0419 14:08:17.578443    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:08:20 minikube kubelet[2314]: I0419 14:08:20.576231    2314 scope.go:117] "RemoveContainer" containerID="8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2"
Apr 19 14:08:20 minikube kubelet[2314]: E0419 14:08:20.576896    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"
Apr 19 14:08:25 minikube kubelet[2314]: I0419 14:08:25.576391    2314 scope.go:117] "RemoveContainer" containerID="35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167"
Apr 19 14:08:25 minikube kubelet[2314]: E0419 14:08:25.576878    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-s2p8q_default(187e51fc-1b75-4598-b2c5-9ebfeaa9ee62)\"" pod="default/wisecow-54fd489786-s2p8q" podUID="187e51fc-1b75-4598-b2c5-9ebfeaa9ee62"
Apr 19 14:08:29 minikube kubelet[2314]: I0419 14:08:29.576425    2314 scope.go:117] "RemoveContainer" containerID="dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7"
Apr 19 14:08:29 minikube kubelet[2314]: E0419 14:08:29.577086    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-r6wpc_default(65ce0807-36ef-4c61-9fb0-2234ea85357a)\"" pod="default/wisecow-54fd489786-r6wpc" podUID="65ce0807-36ef-4c61-9fb0-2234ea85357a"
Apr 19 14:08:29 minikube kubelet[2314]: E0419 14:08:29.579865    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:08:35 minikube kubelet[2314]: I0419 14:08:35.575257    2314 scope.go:117] "RemoveContainer" containerID="8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2"
Apr 19 14:08:35 minikube kubelet[2314]: E0419 14:08:35.575965    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"
Apr 19 14:08:39 minikube kubelet[2314]: I0419 14:08:39.574651    2314 scope.go:117] "RemoveContainer" containerID="35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167"
Apr 19 14:08:39 minikube kubelet[2314]: E0419 14:08:39.575340    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-s2p8q_default(187e51fc-1b75-4598-b2c5-9ebfeaa9ee62)\"" pod="default/wisecow-54fd489786-s2p8q" podUID="187e51fc-1b75-4598-b2c5-9ebfeaa9ee62"
Apr 19 14:08:40 minikube kubelet[2314]: E0419 14:08:40.580521    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:08:43 minikube kubelet[2314]: I0419 14:08:43.574840    2314 scope.go:117] "RemoveContainer" containerID="dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7"
Apr 19 14:08:43 minikube kubelet[2314]: E0419 14:08:43.575484    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-r6wpc_default(65ce0807-36ef-4c61-9fb0-2234ea85357a)\"" pod="default/wisecow-54fd489786-r6wpc" podUID="65ce0807-36ef-4c61-9fb0-2234ea85357a"
Apr 19 14:08:47 minikube kubelet[2314]: I0419 14:08:47.574897    2314 scope.go:117] "RemoveContainer" containerID="8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2"
Apr 19 14:08:47 minikube kubelet[2314]: E0419 14:08:47.576024    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"
Apr 19 14:08:53 minikube kubelet[2314]: I0419 14:08:53.574714    2314 scope.go:117] "RemoveContainer" containerID="35b8b88a72beed77b6b978f2f90d58c9466246210b6b6aca135def5c862c7167"
Apr 19 14:08:53 minikube kubelet[2314]: E0419 14:08:53.575374    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-s2p8q_default(187e51fc-1b75-4598-b2c5-9ebfeaa9ee62)\"" pod="default/wisecow-54fd489786-s2p8q" podUID="187e51fc-1b75-4598-b2c5-9ebfeaa9ee62"
Apr 19 14:08:53 minikube kubelet[2314]: E0419 14:08:53.579904    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with ImagePullBackOff: \"Back-off pulling image \\\"wisecow:latest\\\"\"" pod="default/wisecow-5f4958d5fd-fws4n" podUID="a8291540-6828-4bfc-99f1-43b6291b224b"
Apr 19 14:08:58 minikube kubelet[2314]: I0419 14:08:58.578777    2314 scope.go:117] "RemoveContainer" containerID="dd1435fd6d8e5e371f17fe0ae99324e8753944babef1132bd4b7627f8fd556a7"
Apr 19 14:08:58 minikube kubelet[2314]: E0419 14:08:58.580486    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-r6wpc_default(65ce0807-36ef-4c61-9fb0-2234ea85357a)\"" pod="default/wisecow-54fd489786-r6wpc" podUID="65ce0807-36ef-4c61-9fb0-2234ea85357a"
Apr 19 14:08:59 minikube kubelet[2314]: I0419 14:08:59.575487    2314 scope.go:117] "RemoveContainer" containerID="8cc67d87337e1479039d6d2d2c776357b4f07399a59b625d78bcbcc60cec7af2"
Apr 19 14:08:59 minikube kubelet[2314]: E0419 14:08:59.576338    2314 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"wisecow\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=wisecow pod=wisecow-54fd489786-mvw87_default(81c742bb-ad7d-425c-abf4-33ef955c1dd5)\"" pod="default/wisecow-54fd489786-mvw87" podUID="81c742bb-ad7d-425c-abf4-33ef955c1dd5"

* 
* ==> storage-provisioner [af8b3fbb81c6] <==
* I0419 13:21:50.826167       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0419 13:21:50.834169       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0419 13:21:50.834228       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0419 13:22:08.264245       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0419 13:22:08.264603       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"ae342bce-ae6b-4b99-8f3c-2219c969da7d", APIVersion:"v1", ResourceVersion:"4560", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b2eefad9-1899-4b5b-8aef-e0db1d74365c became leader
I0419 13:22:08.264864       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b2eefad9-1899-4b5b-8aef-e0db1d74365c!
I0419 13:22:08.365584       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b2eefad9-1899-4b5b-8aef-e0db1d74365c!

* 
* ==> storage-provisioner [d3efeeef97e2] <==
* I0419 13:21:35.104594       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0419 13:21:35.107656       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

